[
  {
    "post_number": 1,
    "post_title": "TOON مقابل JSON: هل TOON هو بديل JSON الحقيقي في تعاملات LLMs؟ تحليل مفصل للمزايا والعيوب والاستخدامات المناسبة",
    "post_summary": "الفترة اللي فاتت كل الناس بتتكلم عن صيغة جديدة ظهرت اسمها (TOON) ودي اختصار لـ Token-Oriented Object Notation.\nوفي ناس بتقول إنها ممكن تستبدل الـ JSON لأنها أخف وبتستهلك توكنز أقل وأسرع في التعامل مع الـ LLMs.\nبس من رأيي الكلام ده مش دقيق تعالوا نفهم الموضوع من الآخر\n\nأولاً: إيه هو TOON وليه عامل دوشة الفترة دي؟\nالـ TOON هو طريقة جديدة لتمثيل البيانات بشكل شبه منظم، بس من غير أقواس ولا فواصل ولا علامات تنصيص زي JSON.\nوده مهم لأن كل الأقواس والـ quotes اللي في JSON بتتحسب توكنز، وده بيزود التكلفة، وبيقلل المساحة المتاحة في الـ context، وبيبطأ استجابة الـ model.\n\nفي المقابل، TOON خفيف جداً وبيستهلك توكنز أقل بشكل واضح (ممكن يوصل لتوفير من 30% لـ 50%).\n\nمثال بسيط:\n\nJSON:\n{\n \"name\": \"Omar\",\n \"skills\": [\"AI\", \"ML\"]\n}\n\nTOON:\nname: Omar\nskills:\n - AI\n - ML\n\nطيب… بما إنه خفيف وسهل كده، ليه منستخدموش بشكل أساسي؟\n\nببساطة:\nJSON هو الأكثر استقراراً، وكمان Standard رسمي.\nمدعوم في كل لغات البرمجة، وله Tools، وله Validators، والـ parsing بتاعه ثابت ومتوقع.\n\nأما TOON:\nمفيد فعلاً، بس مش ثابت في كل السيناريوهات.\nملوش Standard رسمي، وكل حد ممكن يكتبه بشكل مختلف.\nالـ parsing بتاعه بيبقى custom.\nمش مناسب للبيانات المعقدة اللي فيها nested structures.\nولو الـ model نفسه ضعيف، ممكن يلخبط في الـ structure\n\nالخلاصة\nTOON ممتاز لو عايز توفر توكنز أو بتتعامل مع بيانات بسيطة ومكررة\nلكن JSON هو الأضمن والأكثر استقراراً لما تيجي تبني نظام حقيقي أو API أو حاجة critical"
  },
  {
    "post_number": 2,
    "post_title": "دليلك الشامل لتعلم LLMs: مراجعة لمحتوى ريبو llm-course على GitHub لجميع المستويات",
    "post_summary": "في ناس كتير كلمتني انها محتارة ازاي تبدا او تكمل ف الـ LLMs فـ mlabonne/llm-course على GitHub بصراحة من أحسن الريبو اللي وقعت عليها بالصدفة واللي هتساعدك في أي مرحلة سواء لسه بتبدأ أو عايز توصل لمستوى كويس\nالريبو ده فيه حاجات قوية جدا زي\nشرح أساسيات الـ LLMs والـ Neural Networks بشكل عملي وبـ Notebooks جاهزة تقدر تشتغل عليها على طول.\nجزء كامل عن RAG Pipelines إزاي تبنيها من الصفر وتستخدمها\nحاجات تانية مهمة جدا زي Fine-Tuning، Quantization، و Evaluation Methods لتطوير models بطريقة كويسة\nكمان بيغطي مواضيع مهمة زي Prompt Engineering، managing memory ، وتحسين أداء models في الproduction\nالجميل إنه مش مجرد شرح نظري لا هتلاقي أمثلة عملية و Notebooks تقدر تجربها بنفسك وتنفذ مشاريع كاملة.\n الريبو: https://lnkd.in/dDtQd9Vt\nhashtag#LLM hashtag#GenerativeAI hashtag#MachineLearning hashtag#DeepLearning hashtag#RAG hashtag#PromptEngineering hashtag#ArtificialIntelligence hashtag#AI hashtag#OpenSource hashtag#DataScience hashtag#LangChain hashtag#FineTuning hashtag#AICommunity hashtag#LearningPath hashtag#Tech"
  },
  {
    "post_number": 3,
    "title": "Top 30+ NLP Use Cases in 2026 with Real-life Examples",
    "summary": "تطور تقنيات الـ NLP في 2026 نحو النمذجة المتخصصة والذكاء الاصطناعي الوكيل (Agentic AI) لتمكين اتخاذ قرارات معقدة وفورية في القطاعات الحيوية."
  },
  {
    "post_number": 4,
    "title": "More ways to build and scale AI agents with Vertex ...",
    "summary": "يوفر Vertex AI Agent Engine بنية تحتية متكاملة لنقل وكلاء الذكاء الاصطناعي من مرحلة التجربة إلى الإنتاج الفعلي بكفاءة عالية وأدوات تقييم دقيقة."
  },
  {
    "post_number": 5,
    "title": "Generative AI with Large Language Models in C# in 2026 - .NET Blog",
    "summary": "تطوير منظومة .NET لتقديم دعم أصلي (Native) ومتكامل لتقنيات الذكاء الاصطناعي التوليدي، مما يعزز أداء وكفاءة تشغيل النماذج الكبيرة (LLMs) برمجياً."
  },
  {
    "post_number": 6,
    "title": "Effective harnesses for long-running agents - Anthropic",
    "summary": "تطوير Anthropic لنظام إدارة مهام الوكلاء الذكيين (Harness) لتعزيز كفاءة العمليات الطويلة عبر تقسيم المهام، ومعالجة الأخطاء، وتقليل استهلاك الموارد."
  },
  {
    "post_number": 7,
    "title": "Microsoft Build 2025: The age of AI agents and building the open ...",
    "summary": "تطور الـ AI Agents من مجرد توليد نصوص إلى أنظمة ذكية ذاتية الإدارة تعتمد على التفكير المنطقي، الذاكرة الطويلة، والتعاون البيني عبر الـ Open Agentic Web."
  },
  {
    "post_number": 8,
    "title": "What are tokens and how to count them? - OpenAI Help Center",
    "summary": "شرح مفهوم الـ Tokenization وآلية عمله في النماذج اللغوية الكبيرة وأهميته في تحسين التكلفة والأداء التقني."
  },
  {
    "post_number": 9,
    "title": "Demystifying evals for AI agents - Anthropic",
    "summary": "تحديات بناء وكلاء الذكاء الاصطناعي تكمن في تطوير نظم تقييم دقيقة (Evaluation Loops) تضمن موثوقية التفكير المنطقي واستقرار الأداء التقني."
  },
  {
    "post_number": 10,
    "title": "Introduction to Large Language Models | Machine Learning",
    "summary": "شرح الآلية التقنية لعمل النماذج اللغوية الكبيرة (LLMs) من التنبؤ الإحصائي والمعالجة الرياضية حتى مرحلة التعلم العميق والضبط الدقيق."
  },
  {
    "post_number": 11,
    "title": "New tech and tools for retailers to succeed in an agentic shopping era",
    "summary": "تحول التجارة الإلكترونية نحو نظام \"Agentic Commerce\" الذي يعتمد على وكلاء ذكاء اصطناعي مستقلين وبروتوكول موحد لاتخاذ قرارات شراء وتنفيذ عمليات متكاملة."
  },
  {
    "post_number": 12,
    "title": "Embeddings | Machine Learning - Google for Developers",
    "summary": "دور الـ Embeddings في تحويل البيانات لمتجهات كثيفة (Dense Vectors) تعبر عن العلاقات الدلالية والأنماط المشتركة لتحسين فهم وكفاءة النماذج."
  },
  {
    "post_number": 13,
    "title": "[1706.03762] Attention Is All You Need - arXiv",
    "summary": "تطوير معمارية الـ Transformer القائمة على الـ Self-Attention لتجاوز قيود النماذج التكرارية وتحقيق كفاءة أعلى في المعالجة المتوازية وفهم السياق."
  },
  {
    "post_number": 14,
    "title": "Vector embeddings | OpenAI API",
    "summary": "شرح دور الـ Vector Embeddings في تحويل النصوص لتمثيلات رياضية تعزز دقة البحث الدلالي (Semantic Search) وأنظمة الـ RAG."
  },
  {
    "post_number": 15,
    "title": "LLMs: Fine-tuning, distillation, and prompt engineering",
    "summary": "توضيح الفروق التقنية بين الـ Fine-tuning والـ Distillation والـ Prompt Engineering كاستراتيجيات لتحسين أداء ونماذج اللغات الضخمة (LLMs)."
  },
  {
    "post_number": 16,
    "title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs",
    "summary": "تطور تقنيات الـ Fine-tuning لتحقيق كفاءة المعلمات (Parameter Efficiency) وضمان دقة الأداء دون فقدان المعرفة الأساسية للنموذج."
  }
]